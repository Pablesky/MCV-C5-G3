import os
from pathlib import Path
import numpy as np
import cv2
import librosa
import pickle
import random
import string
import pandas as pd
import torch
from torchvision.transforms import v2
import soundfile as sf

# This function is generated by chatgpt
def add_noise(text, noise_level=0.1):
    # Determine the number of characters to modify based on noise level
    num_noise_chars = int(len(text) * noise_level)
    
    # Create a list of characters from the text
    text_chars = list(text)
    
    # Modify random characters in the text
    for _ in range(num_noise_chars):
        # Choose a random index to modify
        index = random.randint(0, len(text_chars) - 1)
        # Choose a random modification type: insert, delete, or replace
        modification_type = random.choice(['insert', 'delete', 'replace'])
        
        if modification_type == 'insert':
            # Insert a random character at the chosen index
            random_char = random.choice(string.ascii_letters + string.digits)
            text_chars.insert(index, random_char)
        elif modification_type == 'delete':
            # Delete the character at the chosen index
            del text_chars[index]
        else:  # modification_type == 'replace'
            # Replace the character at the chosen index with a random character
            random_char = random.choice(string.ascii_letters + string.digits)
            text_chars[index] = random_char
    
    # Join the modified characters back into a string
    noisy_text = ''.join(text_chars)
    
    return noisy_text



def augment_audio(audio):
    # Generate random noise with the same length as the audio data
    mean = random.uniform(-0.5, 0.5)
    std = random.uniform(0, 0.5)
    noise = np.random.normal(mean, std, len(audio))  # Mean=0, standard deviation=0.1

    noisy_audio_data = audio + noise
    noisy_audio_data = np.clip(noisy_audio_data, -1, 1)
    return noisy_audio_data

def augment_text(text):
    return add_noise(text, random.uniform(0.1, 0.7))

def augment_image(image):
    transforms = v2.Compose([
        v2.RandomResizedCrop(size=(224, 224), antialias=True),
        v2.ColorJitter(brightness=random.uniform(0, 0.5), contrast=random.uniform(0, 0.5), saturation=random.uniform(0, 0.5), hue=random.uniform(0, 0.5)),#i dont know what values to put
        v2.RandomAffine(degrees=random.uniform(0, 0.5), translate=(random.uniform(0, 0.5), random.uniform(0, 0.5)), scale=(random.uniform(0, 1),random.uniform(0, 1)), shear=random.uniform(0, 0.5),fillcolor=0),
        v2.ToDtype(torch.float32, scale=True),
        v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ])

    return transforms(image)


def augment(name_path, filename):
    img_path = name_path + filename + ".png"
    audio_path = name_path + filename + ".wav"
    text_path = name_path + filename + ".pkl"
    image = cv2.imread(img_path)
    audio, sample_rate = librosa.load(audio_path, sr=None)
    with open(text_path, 'rb') as pkl_file:
        text = pickle.load(text_path)
    au_audio = augment_audio(audio)
    au_img = augment_image(image)
    au_text = augment(text)
    return au_img, au_audio, au_text, sample_rate

    
    
if __name__ == '__main__': 


    root = './'

    train_csv_file = os.path.join(root, 'train_set_age_labels.csv')
    test_csv_file = os.path.join(root, 'test_set_age_labels.csv')
    validation_csv_file = os.path.join(root, 'valid_set_age_labels.csv')
    train_csv = pd.read_csv(train_csv_file)
    test_csv = pd.read_csv(test_csv_file)
    validation_csv = pd.read_csv(validation_csv_file)


    # Get unique ethnicities and genders
    ethnicities = train_csv['Ethnicity'].unique()
    genders = train_csv['Gender'].unique()

    datasets = {'Train': train_csv, 'Test': test_csv, 'Validation': validation_csv}

    information = {}

    for dataset_name, dataset_df in datasets.items():
        print(f"Dataset: {dataset_name}\n")
        for ethnicity in ethnicities:
            for gender in genders:
                # Filter data by ethnicity and gender
                subset = dataset_df[(dataset_df['Ethnicity'] == ethnicity) & (dataset_df['Gender'] == gender)]
                
                # Group the data by 'AgeGroup', and then count the occurrences in each group
                grouped = subset.groupby('AgeGroup').size()
                if dataset_name == 'Train':
                    for index, value in grouped.items():
                        information[(ethnicity, gender,index)] = value


    print(information)
    max_value = max(information.values())
    print("Maximum value in the dictionary:", max_value)

    info_add = {}

    for k,v in information.items():
        info_add[k] = max_value - v


    # ---------------------------------------------------------------------------


    correspondences = {'[07, 13]': 1, '[14, 18]': 2, '[19, 24]': 3, '[25, 32]':4, '[33, 45]':5, '[46, 60]':6, '[61, inf]':7}
    root_dir = '../data/train'

    all_values_zero = all(value == 0 for value in correspondences.values())

    folder_path = "../data/augmented_data"


    if not os.path.exists(folder_path):
        os.makedirs(folder_path)

    counter = 0
    while not all_values_zero:
        for index, row in train_csv.iterrows():
            name = row['VideoName'][:-4]
            key = (row['Ethnicity'], row['Gender'], row['AgeGroup'])
            if key in info_add:
                if info_add[key] > 0:
                    info_add[key] -= 1

                    # get image, audio and text
                    new_path = root_dir + "/" + str(correspondences[row['AgeGroup']]) + "/"
                    new_img, new_audio, new_text, sample_rate = augment(new_path, name)
                    
                    # save image, audio and text
                    new_folder_path = root_dir + "/" + str(correspondences[row['AgeGroup']])
                    if not os.path.exists(new_folder_path):
                        os.makedirs(new_folder_path)
                    #save img
                    cv2.imwrite(new_folder_path + "/" + name + "_" + str(counter) +".png", new_img)

                    #save text
                    with open(new_folder_path + "/" + name + "_" + str(counter) +".pkl", 'wb') as f:
                        pickle.dump(new_text, f)

                    #save audio
                    sf.write(new_folder_path + "/" + name + "_" + str(counter) +".wav", new_audio, sample_rate)

 
            else:
                info_add[key] = max_value
        all_values_zero = all(value == 0 for value in correspondences.values())
        counter += 1